{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №3 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 30 апреля 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 30 апреля, -4 балла после 06:00 7 мая, -6 баллов после 06:00 14 мая, -8 баллов после 06:00 21 мая\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1 (3 балла)**\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "**Задание 2 (3 балла)**\n",
    "Добиться скорости работы на fit не медленнее чем в 10 раз sklearn на данных wine и Speed Dating Data. \n",
    "Для этого используем numpy.\n",
    "\n",
    "**Задание 3 (2 балла)**\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "**Задание 4 (2 балла)**\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw3.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 min_samples_split=2,\n",
    "                 max_depth=None,\n",
    "                 sufficient_share=1.0,\n",
    "                 criterion='gini',\n",
    "                 max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "            self.impurity_measure = self.__gini_imp_measure\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "            self.impurity_measure = self.__entropy_imp_measure\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "            self.impurity_measure = self.__misclass_imp_measure\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features is None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    # обозначим прирост информации IG как IG(x) = I(S) - (phi(x))\n",
    "    # логично, что влиять на S на текущем шаге мы уже не можем\n",
    "    # т.е. имеем const - phi(x), т.е. чтобы максимизировать прирост информации\n",
    "    # нужно минимизровать phi(x). В методах ниже считается как раз phi(x)\n",
    "    # (взвешенная сумма I(S_l) и I(S_r))\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        return 1 - np.sum(l_c ** 2 / l_s.reshape(-1, 1) +\n",
    "                          r_c ** 2 / r_s.reshape(-1, 1),\n",
    "                          axis=1) / (l_s + r_s)\n",
    "\n",
    "    def __gini_imp_measure(self, p_c, p_s):\n",
    "        p_c = p_c.astype(float)\n",
    "        p_s = p_s.astype(float)\n",
    "        return 1 - np.sum((p_c / p_s) ** 2)\n",
    "\n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        return -np.sum(l_c * np.log(l_c / l_s.reshape(-1, 1) +\n",
    "                                    np.finfo(float).eps) +\n",
    "                       r_c * np.log(r_c / r_s.reshape(-1, 1) +\n",
    "                                    np.finfo(float).eps),\n",
    "                       axis=1) / (l_s + r_s)\n",
    "\n",
    "    def __entropy_imp_measure(self, p_c, p_s):\n",
    "        p_c = p_c.astype(float)\n",
    "        p_s = p_s.astype(float)\n",
    "        return -np.sum((p_c / p_s) * np.log(p_c / p_s))\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        return 1 - (np.max(l_c, axis=1) + np.max(r_c, axis=1)) / (l_s + r_s)\n",
    "\n",
    "    def __misclass_imp_measure(self, p_c, p_s):\n",
    "        p_c = p_c.astype(float)\n",
    "        p_s = p_s.astype(float)\n",
    "        return 1 - np.max(p_c / p_s)\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = np.asarray(range(n_feature))\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:np.sqrt(n_feature).astype(int)]\n",
    "\n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = np.asarray(range(n_feature))\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:np.log2(n_feature).astype(int)]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.arange(n_feature)\n",
    "\n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        # \"...количественный признак сортируется по возрастанию...\"\n",
    "        x_sorted, y_sorted = self.__sort_samples(x, y)\n",
    "\n",
    "        # \"...проверяются только те пороги,\n",
    "        # при которых целевой признак меняет значение...\"\n",
    "        borders = np.where(y_sorted[1:] != y_sorted[:-1])[0] + 1\n",
    "\n",
    "        # шаблон говорит о распределении объектов классов по порогам\n",
    "        # (нужен чтобы разом посчитать G_function)\n",
    "        # сначала считаем количество объектов внутри бордеров\n",
    "        classes_in_a_row = borders - np.append(np.array([0]), borders[:-1])\n",
    "\n",
    "        template = np.zeros((borders.shape[0], self.num_class))\n",
    "\n",
    "        # непосредственно указываем какому классу принадлежат объекты внутри\n",
    "        template[np.arange(borders.shape[0]), y_sorted[borders - 1]] = 1\n",
    "\n",
    "        # указываем количество объектов класса, находящихся между бордерами\n",
    "        template *= classes_in_a_row.reshape(-1, 1)\n",
    "\n",
    "        # \"...для каждого порога строится дерево глубины 1...\"\n",
    "        l_c = np.cumsum(template, axis=0)\n",
    "        r_c = np.bincount(y_sorted, minlength=self.num_class) - l_c\n",
    "        l_s = np.sum(l_c, axis=1)\n",
    "        r_s = y_sorted.shape[0] - l_s\n",
    "\n",
    "        # \"...считается насколько снизилась энтропия\n",
    "        # (или неопределенность Джини)...\"\n",
    "        gs = self.G_function(l_c, l_s, r_c, r_s)\n",
    "\n",
    "        # \"...и выбираются только лучшие пороги,\n",
    "        # c которыми стоит сравнивать количественный признак.\"\n",
    "        idx = gs.argmin()\n",
    "        before_border = x_sorted[int(l_s[idx])-1]\n",
    "        after_border = x_sorted[int(l_s[idx])]\n",
    "        best_threshold = np.mean([before_border, after_border])\n",
    "        # © статья ODS по деревьям на хабре,\n",
    "        # сильно помогла вспомнить что было на лекции\n",
    "        return gs[idx], best_threshold\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth):\n",
    "        if (depth == self.max_depth or\n",
    "                x.shape[0] <= self.min_samples_split or\n",
    "                np.unique(y).shape[0] == 1):\n",
    "            self.tree[node_id] = (self.__class__.LEAF_TYPE,\n",
    "                                  np.bincount(y).argmax(),\n",
    "                                  np.bincount(y).astype(float) / y.shape[0])\n",
    "            return\n",
    "\n",
    "        features_ids = self.get_feature_ids(x.shape[1])\n",
    "\n",
    "        # чтобы не было проблем с шейпами и можно было\n",
    "        # по argmin вытащить нужный id фичи\n",
    "        feature_threshold_pairs = np.zeros((2, x.shape[1])) + np.inf\n",
    "\n",
    "        feature_threshold_pairs[:, features_ids] = \\\n",
    "            np.apply_along_axis(self.__find_threshold,\n",
    "                                0,\n",
    "                                x[:, features_ids],\n",
    "                                y)\n",
    "\n",
    "        # нашли как будем делить\n",
    "        best_split_feature_id = \\\n",
    "            np.argmin(feature_threshold_pairs[0, :])\n",
    "        best_split_feature_threshold = \\\n",
    "            feature_threshold_pairs[1, best_split_feature_id]\n",
    "        # поделили\n",
    "        x_l, x_r, y_l, y_r = self.__div_samples(x,\n",
    "                                                y,\n",
    "                                                best_split_feature_id,\n",
    "                                                best_split_feature_threshold)\n",
    "\n",
    "        # по пути считаем feature importances\n",
    "        self.feature_importances_[best_split_feature_id] += \\\n",
    "            self.impurity_measure(np.bincount(y),\n",
    "                                  np.asarray(y.shape[0])) - \\\n",
    "            (float(y_l.shape[0]) *\n",
    "                self.impurity_measure(np.bincount(y_l),\n",
    "                                      np.asarray(y_l.shape[0])) +\n",
    "             float(y_r.shape[0]) *\n",
    "                self.impurity_measure(np.bincount(y_r),\n",
    "                                      np.asarray(y_r.shape[0]))) / \\\n",
    "            float(y.shape[0])\n",
    "\n",
    "        # вдруг получилось так, что можно положить все в листовую ноду\n",
    "        if x_l.shape[0] == 0 or x_r.shape[0] == 0:\n",
    "            self.tree[node_id] = (self.__class__.LEAF_TYPE,\n",
    "                                  np.bincount(y).argmax(),\n",
    "                                  np.bincount(y).astype(float) / y.shape[0])\n",
    "            return\n",
    "\n",
    "        # записали то как поделили\n",
    "        self.tree[node_id] = (self.__class__.NON_LEAF_TYPE,\n",
    "                              best_split_feature_id,\n",
    "                              best_split_feature_threshold)\n",
    "\n",
    "        self.__fit_node(x_l, y_l, 2 * node_id + 1, depth + 1)\n",
    "        self.__fit_node(x_r, y_r, 2 * node_id + 2, depth + 1)\n",
    "\n",
    "        return\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.feature_importances_ = np.zeros(x.shape[1])\n",
    "        self.num_class = np.unique(y).shape[0]\n",
    "        self.__fit_node(x, y, 0, 0)\n",
    "        self.feature_importances_ /= y.shape[0]\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 ms, sys: 178 µs, total: 1.97 ms\n",
      "Wall time: 1.18 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 ms, sys: 0 ns, total: 31.1 ms\n",
      "Wall time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487179487179486"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487179487179486"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data, feature):\n",
    "    for code in np.sort(data[feature].unique()):\n",
    "        data[feature + '=' + str(code)] = \\\n",
    "            (data[feature] == code).astype(np.float)\n",
    "    data = data.drop([feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (8378, 195)\n",
      "df_pair.shape = (3999, 157)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Speed Dating Data.csv', sep=',', encoding='latin1')\n",
    "print('df.shape =', df.shape)\n",
    "\n",
    "df = df.iloc[:, :97]\n",
    "df = df.drop(['id', 'idg', 'condtn', 'round', 'position',\n",
    "              'positin1', 'order', 'partner', 'age_o',\n",
    "              'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'dec_o',\n",
    "              'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o',\n",
    "              'shar_o', 'like_o', 'prob_o', 'met_o'], axis=1)\n",
    "\n",
    "df = df.dropna(subset=['age'])\n",
    "\n",
    "df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "df = df.drop(['field'], axis=1)\n",
    "one_hot(df, 'field_cd')\n",
    "df = df.drop(['field_cd'], axis=1)\n",
    "\n",
    "df = df.drop(['undergra'], axis=1)\n",
    "\n",
    "df.loc[:, 'mn_sat'] = \\\n",
    "    df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "\n",
    "df.loc[:, 'tuition'] = \\\n",
    "    df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "\n",
    "one_hot(df, 'race')\n",
    "\n",
    "df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "df = df.drop(['from', 'zipcode'], axis=1)\n",
    "\n",
    "df.loc[:, 'income'] = \\\n",
    "    df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "one_hot(df, 'goal')\n",
    "\n",
    "df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "df = df.drop(['career'], axis=1)\n",
    "one_hot(df, 'career_c')\n",
    "\n",
    "df = df.drop(['sports', 'tvsports', 'exercise', 'dining',\n",
    "              'museums', 'art', 'hiking', 'gaming', 'clubbing',\n",
    "              'reading', 'tv', 'theater', 'movies', 'concerts',\n",
    "              'music', 'shopping', 'yoga'], axis=1)\n",
    "\n",
    "df = df.drop(['expnum'], axis=1)\n",
    "\n",
    "df.loc[:, 'temp_totalsum'] = \\\n",
    "    df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1',\n",
    "               'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "\n",
    "df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1',\n",
    "           'amb1_1', 'shar1_1']] = \\\n",
    "    (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1',\n",
    "                'fun1_1', 'amb1_1', 'shar1_1']].T /\n",
    "     df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "\n",
    "df.loc[:, 'temp_totalsum'] = \\\n",
    "    df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1',\n",
    "               'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "\n",
    "df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1',\n",
    "           'amb2_1', 'shar2_1']] = \\\n",
    "    (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1',\n",
    "                'fun2_1', 'amb2_1', 'shar2_1']].T /\n",
    "     df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "\n",
    "df = df.drop(['temp_totalsum'], axis=1)\n",
    "\n",
    "for i in [4, 5]:\n",
    "    feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "            'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "            'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "    if i != 4:\n",
    "        feat.remove('shar{}_1'.format(i))\n",
    "    df = df.drop(feat, axis=1)\n",
    "\n",
    "df = df.drop(['wave'], axis=1)\n",
    "\n",
    "df_male = \\\n",
    "    df.query('gender == 1')\\\n",
    "    .drop_duplicates(subset=['iid', 'pid'])\\\n",
    "    .drop(['gender'], axis=1)\\\n",
    "    .dropna()\n",
    "\n",
    "df_female = \\\n",
    "    df.query('gender == 0')\\\n",
    "    .drop_duplicates(subset=['iid'])\\\n",
    "    .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1)\\\n",
    "    .dropna()\n",
    "\n",
    "df_female.columns = df_female.columns + '_f'\n",
    "\n",
    "df_female = df_female.drop(['pid_f'], axis=1)\n",
    "\n",
    "df_pair = df_male.join(df_female.set_index('iid_f'),\n",
    "                       on='pid',\n",
    "                       how='inner')\n",
    "df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "print('df_pair.shape =', df_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 0 ns, total: 102 ms\n",
      "Wall time: 102 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 0 ns, total: 1.34 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5747696669029057"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.551264367816092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "['int_corr', 'age', 'income_f', 'sinc1_1_f', 'exphappy', 'attr3_1', 'fun2_1_f', 'attr1_1', 'intel2_1', 'shar2_1_f']\n",
      "\n",
      "MyDecisionTreeClassifier\n",
      "['int_corr', 'income', 'shar2_1_f', 'intel2_1_f', 'intel1_1', 'tuition_f', 'fun2_1_f', 'amb3_1_f', 'date_f', 'exphappy_f']\n"
     ]
    }
   ],
   "source": [
    "print('DecisionTreeClassifier')\n",
    "print(list(df_pair.columns[1:][np.flip(np.argsort(clf.feature_importances_))])[:10])\n",
    "print()\n",
    "print('MyDecisionTreeClassifier')\n",
    "print(list(df_pair.columns[1:][np.flip(np.argsort(my_clf.feature_importances_))])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 36, 'min_samples_split': 8, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "\n",
    "params = {\n",
    "    'max_depth': [6, 12, 18, 24, 36, 48],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10, 20, 40],\n",
    "    'n_estimators': [1, 5, 10, 15, 20, 40]\n",
    "}\n",
    "\n",
    "searcher = GridSearchCV(forest, params, cv=3)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "print(searcher.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5772994129158513\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_depth=36,\n",
    "                                min_samples_split=8,\n",
    "                                n_estimators=40)\n",
    "forest.fit(X_train, y_train)\n",
    "print(f1_score(y_pred=forest.predict(X_test),\n",
    "               y_true=y_test,\n",
    "               average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
